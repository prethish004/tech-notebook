"""
âœ… RAG INSIDE THE AGENT
âœ… LLM-BASED DECISIONS (no hard-coded rules)
âœ… Single, full, working Python file
âœ… Very detailed explanation of WHY each part exists

This is the foundation of LangGraph / CrewAI / AutoGPT.

ğŸ§  What We Are Building (High Level)

An agent that decides (using an LLM)
when to retrieve knowledge (RAG)
when to reason internally
how to plan
how to act
when to stop

ğŸ— FINAL ARCHITECTURE
USER GOAL
   â†“
LLM PLANNER (creates steps)
   â†“
FOR EACH STEP:
   â”œâ”€â”€ LLM decides: Need RAG or not?
   â”œâ”€â”€ If yes â†’ RAG Retriever
   â”œâ”€â”€ If no â†’ Reasoning
   â”œâ”€â”€ Store result in Memory
   â†“
LLM SYNTHESIS (final answer)

ğŸ“¦ INSTALL REQUIREMENTS
pip install openai faiss-cpu sentence-transformers numpy


âš ï¸ Use an OpenAI-compatible model (OpenAI / Azure / Ollama API)
AGENTIC AI + RAG + LLM DECISION MAKING
------------------------------------
This is a REAL agentic RAG system.

Capabilities:
- LLM-based planning
- LLM-based tool decision (RAG vs reasoning)
- Vector search (RAG)
- Memory
- Controlled execution loop
"""

import openai
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

# ==============================
# ğŸ”‘ CONFIG
# ==============================

openai.api_key = "YOUR_API_KEY"

LLM_MODEL = "gpt-3.5-turbo"

# ==============================
# ğŸ“š KNOWLEDGE BASE (RAG)
# ==============================

DOCUMENTS = [
    "Agentic AI refers to systems that can plan, act, observe, and iterate toward a goal.",
    "RAG stands for Retrieval Augmented Generation.",
    "RAG improves LLM responses by grounding them in external knowledge.",
    "Python was created by Guido van Rossum.",
    "FAISS is a vector database for similarity search."
]

# ==============================
# ğŸ§  EMBEDDINGS + VECTOR DB
# ==============================

embedder = SentenceTransformer("all-MiniLM-L6-v2")
doc_embeddings = embedder.encode(DOCUMENTS)

index = faiss.IndexFlatL2(doc_embeddings.shape[1])
index.add(doc_embeddings)

def rag_retrieve(query, k=2):
    """Retrieve relevant documents using vector similarity"""
    q_emb = embedder.encode([query])
    _, I = index.search(q_emb, k)
    return [DOCUMENTS[i] for i in I[0]]

# ==============================
# ğŸ§  MEMORY
# ==============================

class AgentMemory:
    def __init__(self):
        self.history = []

    def add(self, item):
        self.history.append(item)

    def get_context(self):
        return "\n".join(self.history[-6:])

# ==============================
# ğŸ¤– LLM HELPERS
# ==============================

def call_llm(system, user):
    response = openai.ChatCompletion.create(
        model=LLM_MODEL,
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": user}
        ]
    )
    return response.choices[0].message.content.strip()

# ==============================
# ğŸ—º PLANNER (LLM-BASED)
# ==============================

def llm_planner(goal):
    """
    WHY:
    - Removes hard-coded logic
    - Allows dynamic planning
    """
    system = "You are an expert planner AI."
    user = f"""
Goal: {goal}

Create a step-by-step plan.
Return steps as numbered list only.
"""
    plan_text = call_llm(system, user)
    return [step.split(". ", 1)[1] for step in plan_text.split("\n") if "." in step]

# ==============================
# ğŸ§­ TOOL DECISION (LLM-BASED)
# ==============================

def decide_need_rag(step):
    """
    LLM decides whether retrieval is needed
    """
    system = "You decide whether external knowledge is required."
    user = f"""
Step: {step}

Answer ONLY one word: YES or NO
"""
    decision = call_llm(system, user)
    return decision.strip().upper() == "YES"

# ==============================
# âš™ EXECUTOR (AGENT CORE)
# ==============================

def execute_step(step, goal, memory):
    print(f"\nğŸ§  STEP: {step}")

    need_rag = decide_need_rag(step)
    print("ğŸ” Need RAG?", need_rag)

    if need_rag:
        docs = rag_retrieve(step)
        observation = "\n".join(docs)
        print("ğŸ“š RAG RESULT:", observation)
    else:
        system = "You are a reasoning assistant."
        observation = call_llm(system, step)
        print("ğŸ§  REASONING RESULT:", observation)

    memory.add(observation)

# ==============================
# ğŸ§ª FINAL SYNTHESIS
# ==============================

def finalize_answer(goal, memory):
    system = """
You are a senior AI assistant.
Answer ONLY using the context.
If unsure, say I don't know.
"""
    user = f"""
Context:
{memory.get_context()}

Goal:
{goal}
"""
    return call_llm(system, user)

# ==============================
# ğŸš€ AGENT CONTROLLER
# ==============================

def agentic_rag(goal):
    print("\n==============================")
    print("ğŸ¯ GOAL:", goal)
    print("==============================")

    memory = AgentMemory()

    # 1ï¸âƒ£ PLAN
    plan = llm_planner(goal)
    print("\nğŸ—º PLAN:")
    for i, step in enumerate(plan, 1):
        print(f"{i}. {step}")

    # 2ï¸âƒ£ EXECUTE STEPS
    for step in plan:
        execute_step(step, goal, memory)

    # 3ï¸âƒ£ FINAL ANSWER
    print("\nâœ… FINAL ANSWER:")
    answer = finalize_answer(goal, memory)
    print(answer)

# ==============================
# â–¶ RUN
# ==============================

if __name__ == "__main__":
    agentic_rag("Explain Agentic AI and how RAG improves it")
"""
ğŸ§  WHY THIS IS IMPORTANT (READ THIS)
ğŸ”¹ Why embed RAG inside the agent?

Because:
  Agent decides WHEN to retrieve
  Not every step needs search
  Saves tokens & time
  Improves accuracy
This is how ChatGPT tools work internally.

ğŸ”¹ Why LLM-based decisions?
Hard rules âŒ
Real world = messy â—
LLM decisions allow:
  Dynamic behavior
  New domains
  Less code changes

âœ… WHAT YOU HAVE NOW
Feature	Status
Agentic control loop	âœ…
RAG integrated	âœ…
LLM planning	âœ…
LLM tool decision	âœ…
Memory	âœ…
Grounded answers	âœ…
"""

